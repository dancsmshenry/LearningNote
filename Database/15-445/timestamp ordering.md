背景

- 对于2PL中的锁，我们想到的就是锁会影响性能（所以2PL也是一种悲观的思路）
- 假设未来事务之间会发生竞争，于是提前加锁
- 在问题发生之前，阻止问题发生
- 乐观的想法就是基于时间戳顺序的并发控制



- 根据每个事务给它赋予时间戳，根据事务的时间戳决定它的顺序
- 如果Ti的时间戳小于Tj的时间戳，那么数据库要保证，相当于Ti完全先发生，Tj完全后发生



- 每个事务都会给它一个独一无二的时间戳
- 使用不同的实现方法会有不同的值
- 实现方法：
  - 系统时钟（os的时钟）有时会出问题，电脑的时间不完全精确（手机电脑的时钟都是不精确的，之所以如此精确是因为同步机制，即每隔一段时间进行时间的校验）；有可能时间A的来了一个事务a，系统时钟校验后居然校验到了时间A，而此时事务b来了，二者的时间戳变一样的了
  - logical counter（逻辑计数器）；简单计数实现先后（分布式系统的问题在于很难在多个机子之间做计数器的校准；如果两个结点同时接收到一个事务，那么counter的校准就出现了问题）
  - hybrid（系统时钟+logical counter，hlc）





# basic timestamp ordering protocol

- 每行记录都要附上时间戳
- 一个是读时间戳（W-TS，上次读这个行记录的事务的时间戳），另一个是写时间戳（R-TS，上次写这个行记录的事务的时间戳）
- 每次读写数据的时候，都要检查本事务的时间戳和当前事务上所写的时间戳
- 不能操作未来的数据





## basic T/O READS

- 假如此时读一个数据，发现上次修改该事务的id（即写时间戳）大于当前的id，拿当前事务就不能读该数据
  - 就得回滚，然后重新来一个事务，因为不能读到未来的数据
- 如果小于的话，就是合法的，就可以读；读完之后要更新当前的读时间戳（则当前的读时间戳变为max（写时间戳，当前事务的时间戳））；然后还要把数据拷贝到本地做一个副本（因为可能后面又有事务会修改该数据，所以要把它留下来本地，作为一个副本）
  - 因为可以读到之前现在的数据



## basic T/O WRITES

- 如果当前事务的时间戳小于当前记录的读时间戳（不能更新未来的数据），或者小于当前记录的写时间戳（）
  - 就得回滚，重新开始新的事务
- 否则，就允许当前事务写数据（逻辑：只有当前事务的时间戳大于当前记录的读和写时间戳，才可以写数据）
  - 可以更新和写数据了
  - 同时，也要将写的一个版本放到本地一份，以便可重复读



- 其实，时间戳说的是事务由先后id，实际上就是用这个先后的id来维护串行化的同时，找到一些并发度高的解
- 这里有一个很精彩的推断，就是两个事务同时对一个数据进行写入，时间戳的发现实际上真正落盘的只有一个事务的数据，所以才有了上面的结论
  - 就像example2一样，T1的数据不往数据库里面写，而是可以在本地存一下，后续本事务就在这上面读即可
  - 即可以做到数据好像写了，但是后来被覆盖了



## thomas write rule

- 如果写的事务的时间戳是小于数据的读时间戳的话，那只能abort
  - 因为这个数据未来要有人读，我就不能先在改变
- 但，如果才是事务的时间戳是小于数据的写时间戳的话
  - 这里就说明未来有人要写这个数据
  - 就未来有一个事务改了这个数据，我们可以做到的就是假装数据写了
  - 即此时可以不修改数据继续往下走（反正这里未来会被覆盖掉）





总结

- 不存在死锁
- 缺点：非常长的事务就会导致，我碰到什么数据，它都被未来的事务修改了，这样的话就又不能读又不能写；重启的话也要走那么久，无济于事
- 即长事务会导致饥饿



## recoverable schedules

- 有一个很大的问题就是，如果按照时间戳顺序，后续读的数据是要前面的事务的基础上，问题是，前面的事务可能发生了回滚，导致后面的数据出现了问题
- 因为事务的恢复是按照顺序恢复的，即要先恢复前面的事务再恢复后面的事务，但是你前面的事务都回滚了，后面的事务又是基于前面的事务的，那后面的事务就无法回滚了
- 即basic top，是无法恢复的





- 还有一个问题就是，这种算法会复制一份当前的数据，但如果我们要进行全表扫描的话，那么就会造成把整个数据库的数据都复制了一份，浪费空间了属于是



- 如果当前的事务比较短，事务的竞争比较少，那么无锁的方法是最好的



# Optimistic Concurrency Control

- 乐观的并发控制
- 基于时间戳的
- 这个名字其实是命名问题，即时间戳不上锁的都是乐观的并发控制

- DBMS创建一个私人的空间用于存放数据记录，每次的修改都是放在这个私人空间中的（所有的修改都临时的放到本地）
- 每次提交会和原来数据库的数据进行对比，如果没有冲突的话会把所有的数据全部提交





## read phase

- 将数据从DBMS中读到本地，没有写，要写的东西临时记在本地，最后再写
- 是指对于数据库本身来说是只读
- 记录要读的数据和要写的数据，把这些数据都放到本地（都要拷贝到本地，为了实现可重复读），放在一个私有的空间里面
- 应该是在操作数据之前发生的



## validation phase

- 是在commit的时候发生的
- 校验阶段，检测是否有冲突
- 事务准备好了，要提交了的时候，把要写的数据和别的事务进行比较，如果没有问题，就进入下一个阶段
- 也会分配一个时间戳，校验阶段就是要保证小时间戳先发生，大的后发生
  - 注意，是在校验阶段的时候才会赋予时间戳




### backward validation

- 向前校验（向已经发生了提交了的事务的数据进行校验）
- 比较一下，看有没有成环的冲突：没有的话就提交，有的话当前事务就要abort本身



### forward validation

- 校验未来的事务，可是未来的事务还有一部分没有做
- 所以这里是校验未来事务和当前事务交叠的部分，还没发生的部分无法校验
- 所以发生了冲突，这里可以灵活的选择是abort当前事务，还是哪些还在进行的事务



而事务和事务之间时间上的关系又分为三种关系

- 关系一，事务A和事务B是真的串行化的，那就直接commit即可
- 关系二，
- 关系三

PS：这一部分真的不好理解....（校验阶段的三种情况有点乱..）



## write phase

- 如果校验成功，就把本地的数据写入到磁盘中
- 有些DBMS会锁全表（一般来说执行的时间很短，都是可以接受的）



思考（observation）

- 冲突是比较少的场景是ok的（冲突太多，回滚很难受）
- 所有的事务只读最好
- 或者事务之间没有冲突也最好（数据库比较大的时候，查询对于整个表来说是均衡的，没有啥热点数据），这样的情况下冲突会比较小，就可以考虑OCC的并发控制协议（因为在冲突比较小的情况下，使用mutex就是浪费）



缺点

- 复制数据还是存在开销
- 校验这一步是一个非常复杂的逻辑
- 写数据会锁全表，顺序写表，不能并发，所以并发度低
- abort的成本会比2PL的更大（2PL有死锁检测，但OCC会在发现死锁的时候把整个事务回滚，资源浪费严重）



# partition based T/O

- 将数据库分割为不相交的子集，即水平分区
- 使用时间戳对事务进行排序，以便在每个分区上串行执行
  - 只检查运行在同一个分区的事务之间的冲突
- 事务根据它们到达DBMS的时间分配时间戳，分区由单个锁保护
  - 每个事务在它需要的分区上排队，如果该事务在分区队列中拥有最小的时间戳，那么就可以得到锁
  - 事务启动后，它可以读取写入所有分区的锁



- 有点像每个分区都维护一个队列，分区按照这个队列的顺序执行；如果发生了越界执行，就要abort



## read

- 事务可以在锁定的分区上读取它们想要的任何内容
- 如果事务试图读取一个他没有锁的分区，将会被abort



## write

- 所有的更新都是在原地发生
  - 在内存中维护一个单独的缓冲区来撤销事务的修改
- 如果事务试图修改一个他没有锁的分区，将会被abort



- 面对一下情景，这种协议是迅速高效的
  - 在txn启动之前，DBMS知道它需要哪些分区
  - 大多数(如果不是全部)txns只需要访问单个分区





- 面对上述的两种并发协议，2PL和OCC在某些情况下会出现问题（幻读）
- 发现此前写的数据都是写现成的数据，但是如果是insert和delete的话，就会出现问题
  - 所以之前一直对事务的隔离级别以及相关联的内容有误解，就是因为这个



# isolation levels

幻读 phantom problem

- OCC和2PL都没有考虑到这个问题
- 准确来说就是，执行两条sql，前后读出的内容不一致，好像出现了幻觉，而这里的原因是因为在这期间发生了新的数据的insert和delete
- 和不可重复读的情况不同，不可重复读是指数据发生了修改，而幻读是指数据直接变样了
  - 比如说查找当前的max值，第一次是A，然后其他的事务添加了数据B，使得当前的max变为了数据B，导致第二次读的时候返回的是数据B，这就是幻读
  - 而不可重复读是指，针对同一条数据，第一次数据记录A的第一列是1，然后其他的事务修改了这一列，改为了2，导致第二次读的时候返回的是2，这就是不可重复读
  - 而脏读，就是未提交的事务的数据了..
- 第二次读到了第一次不存在的数据
- 为什么2PL解决不了？因为锁只能锁当前的数据，控制不了新的数据的插入，而上表锁并发度会下降



解决方案

- 方法一：re-execute scans
  - 记录下事务所有能够出现幻读的地方（查询max，min，范围查询）
  - 在commit前再部分扫描表格
  - 缺陷：多次内存的读取，浪费资源，性能差
- 方法二：predicate locking 谓词锁
  - 给sql的where谓词加共享锁
  - 给UPDATE INSERT DELETE上加独占锁
  - 比如上面的例子，where上加了共享锁，那么后续insert要加独享锁就会出现问题
- 方法三：index locking 索引锁
  - 给索引加锁（问题是当前的谓词没有索引怎么办，那么就要加表锁）
  - 就是不能让他添加删除新的数据
- mysql的实现：间隙锁
  - mysql将数据和数据之间的间隙也认为是数据，在类似max的查询中，将数据之间的间隙给上锁
  - 如果间隙有索引的话，那就直接锁当前数到max之间的间隙
  - 比如说第一次找到max为20，就把20-max之间给上一个间隙锁



- 思考：我们做了那么多的事务并发协议，都是为了能够将事务的执行顺序，等效的转化为串行化
- 但是我们的业务真的需要这么严格的隔离级别吗...
