两种适合磁盘的索引结构：就地更新（B+树，随机读和顺序读是非常高效的，因为随机读有二分查找的支持，范围查找也能够有底层链表的支持；对于顺序读，不需要转到寻道寻址，不需要过多的转动磁头；对于随机读，有缓存的优化，pre read；适合读多写少的常见）和非就地更新



但是其实，有些场景是写密集型的情况（比如说raft的log，webserver的log），如何高效的写日志

spark的计算密集型的场景，mapreduce等，都是写入密集型的情况



其实机械磁盘的顺序写性能上是很不错的，因为不需要转动磁头

但是随机写的性能会非常的差



背景：随机写的性能很差，需要优化 ->> 随机写转化为顺序写



想到的一种解决方案：写追加

缺点：如果有一个数据非常非常早就写入了，后续基本没有修改过，但是如果某一时刻需要读取这个数据，那么就需要从后往前的遍历，性能就会很差

高写吞吐量，较低的读延迟



两个缺点：读取的延迟很高（for循环整个log）；浪费空间（同一个kv可能会出现多次；解决办法，gc垃圾收集，发现无效的kv或者国企的kv，就把它给删除）

发现：如果一遍一直写入数据更新数据，而另一边又一直在gc，在查找的时候就很难判断kv是否是正确的版本

即流式的，动态的，就很难判断一个kv是否为最新的版本

一个好的思路：将log分段

背景：文件系统读大文件的缓存支持上也不是很友好（所以要把log分为一小段一小段）

实操：只有一个log文件是不断写入的，当这个log文件达到一定的阈值的时候，就会把它给刷盘压缩

换句话说，当log文件的大小达到阈值以后，就把它设定为不可写的，接着放入磁盘，并让后台线程对其进行压缩gc

后台的gc线程就可以维护一个全局的map，从后往前遍历：如果kv数据没有出现过，就直接放入；如果出现过，就跳过该数据



补充：对于kv引擎，不仅要支持get，set，还要支持range read（范围顺序读取，比如读取某天24hours的数据）

比较常见的场景：获取当前某一天数据的所有的日志（range）

一个思路：假设每一个log的文件块都是已经排好序了的，那么就可以将两个log文件块进行归并排序

然后有个比较好的归并方法：因为我们假设每个log文件的数据在内部是已经排好序了的，如果两个文件的k范围有重合，就需要对其进行排序，把得到的文件放到下一层，否则就不管它：

比如说有三个文件，log1中数据的范围是10到101，log2中的数据范围是5到100，log3中的数据范围就是103到200

我们发现，只需要合并log1和log2就行了，log3不需要额外的合并

这样做能够有效的减少归并的次数，这样也能够有效的用二分查找的方式进行查找

比如说此时合并后得到的数据范围是5到101，而log3就是103到200，这样就能够快速的查找

这是gc的过程，现在再说一下优化读取性能（就像上边说的，读取某些冷门数据，可能需要O（n）级别的时间复杂度）

我们在内存部分维护一个跳表，每次的kv插入都放入到跳表中

当跳表到达一定的长度以后，就将数据dump到盘中

因此需要在内存中将得到的数据缓存起来，从而减少dump的次数（只有在积累了一批数据以后才会dump数据到磁盘中）（也就是一般说的批处理）

问题：什么时候才会将数据dump到磁盘中（和gc类似，我们是在写入的过程中将数据dump下来，但是很难划分一个界限，去告诉他什么时候dump停下来，因为dump的速度没有数据写入的速度快；可能会导致文件越写越大）解决办法：在内存中维护两个跳表，一个是活跃跳表，接收外在的写入请求，阈值达到一定程度的时候，变为不可变的跳表；接着创建一个新的跳表接收新的写请求，而此前不可变的跳表就需要被后台线程dump到磁盘中

第二个问题：数据写入了，但是没有持久化成功（比如说你写到了内存上，但是数据没有落盘，最终导致的结果就是数据丢失）

解决办法：给定一个预写日志，即WAL（我们的实践中不需要对日志手段进行优化，即对其写入性能没有过多的要求）



新的问题：我们有可能读取的数据是非常远古的数据，以致于该数据没有存储在当前的内存中，那应该如何的去读取（即在最老的log文件中出现过一次）

那么这里的时间复杂度就是O(n)，随着数据量的增大而线性增长的

所以应该如何去优化呢？引入leveldb的重要思想，即将日志段文件分层分级 sstable

经过第一次归并的文件，数据之间一定不会有重叠的情况出现

所以leveldb为了解决这种O（n）级别的时间复杂度的查询，选择牺牲一定的内存空间和时间，提高查询的性能：

假设一开始落盘的log文件都是在第零层

紧接着，当log文件的数量达到一定的阈值的时候，就将其进行merge操作（这里的merge操作参考的是上述的merge，即log文件本身是有序的，但是不同的log文件数据范围上会有重叠的情况，因此需要对其进行归并）

merge得到的log文件放到第一层

如果第一层的数据文件也慢慢的变多了，就先将一个log文件放入第二层，紧接着再对第一层的数据文件进行merge

（有点不是很能理解sstable的流程？？）

上述就是sstable的流程，而我们需要限制每一层log文件的数量，同时也要限制level的层数

同时还要维护一个索引，记录每一层的数据范围，方便后续数据的查找

比如说我们假定sstable的第零层只能有8个文件，level的层数有6层

那么，对于一个数据的查找，首先会在这8个文件中查找，如果这8个文件没有的话

就需要去下一层索引文件中查找数据是否在其中（索引文件中可以用二分查找进行查找的）

所以说，对于一个数据的查找，最坏的时间复杂度就是要查找8+6，即14个文件



但还是有问题：尽管我们不断地merge log文件，但是架不住数据量的飙升，导致地结果就是即使merge了，最后剩下地log文件还是越来越大

需要对数据地排布进行布局：元数据区，索引区，数据段（这就是sstable的存储格式）

元数据区：布隆过滤器（存储空间小，但是也存在假阳性；给定一个k，用布隆过滤器进行判断，如果不存在的话就直接跳过就行了；用这个可以极大的减少文件的读取，从而实现性能上的优化，因为不用把数据全部读取就可以判断数据在不在里面了）



因为WAL的日志是不断增长的，我们系统每次崩溃的时候如果都从头开始恢复的话，性能就很差，所以需要checkpoint进行优化

周期性对kv数据库进行快照

即：需要记录那些日志对应的操作数据是已经被写入到磁盘中的了，那些是还没有写入到磁盘中的，只保留后者即可